<!-- <!doctype html> -->
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <title>LLMs for Robotics</title>

  <link rel="stylesheet" href="stylesheets/my_styles.css">
  <link rel="stylesheet" href="stylesheets/pygment_trac.css">
  <meta name="viewport" content="width=device-width">
  <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$','$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };

    window.addEventListener('load', (event) => {
        document.querySelectorAll("mjx-container").forEach(function(x){
          x.parentElement.classList += 'has-jax'})
      });
  </script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
<div class="my_wrapper">
  <h1>Large Language Models for Robotic Manipulation</h1>
  <div class="toc">
    <details >
    <summary accesskey="c" title="(Alt + C)">
      <span class="details">Table of Contents</span>
    </summary>

    <div class="inner">
      <ul>
      <li><a href="#intro">Introduction</a></li>
      <li><a href="#approach">Language in Robotic Manipulation</a></li>
        <ul>
          <li><a href="#conditioned">Language Conditioned Control</li>
          <li><a href="#middleware">Language as Middleware</li>
        </ul>
      <li><a href="#discussion">Discussion</a></li>
      <li><a href="#case_study">Case Study</a></li>
      <li><a href="#references">References</a></li>
      </ul>
    </div>
    </details>
  </div>

  <p>
    <em>This project was developed as part of the PhD course in 
      <a class="one" href="https://fenix.tecnico.ulisboa.pt/cursos/deec/disciplina-curricular/1127428915200109">Deep Structured Machine Learning</a>.
      The purpose of the project was to write a review paper on recent methods employing 
      large language models for robotic manipulation. You can find the full review 
      <a class="one" href="assets/project_llms/llms_for_robotic_manipulation.pdf">here</a>. In this post, I will briefly discuss the different 
      approaches and showcase a simple implementation of one of the discussed methods. 
      The project was completed in the spring semester of 2023, since then, new approaches 
      have been proposed that are not discussed here.
    </em>
  </p>

  <br> <h1 id="intro">Introduction<a hidden class="anchor" href="#intro">#</a></h1>
  <p> 

  The recent advancements in <a class="one" href="https://en.wikipedia.org/wiki/Large_language_model">large language models (LLMs)</a> have
  propagated in many fields from virtual assistants to physical
  robots. The integration of LLMs with robotic agents promises 
  to yield robots capable of planning and executing complex tasks
  that can operate in unstructured environments. There are several 
  advantages to using LLMs in the context of robotics: 1) having 
  robots that can understand natural language instructions would 
  allow humans to interact with robots more intuitively, 
  2) pre-trained LLMs can leverage the vast amounts of data available
  online to encode knowledge of multiple instances for which gathering
  robotic data would be impossible, and 3) using the emergent 
  abilities of LLMs, such as few-shot prompting and chain-of-thought 
  reasoning, robotic agents can potentially adapt to new environments 
  and tasks more easily than traditional robotic systems.
  Below we will discuss some recent approaches to using LLMs
  in robotic manipulation scenarios and present a case study 
  for planning.

<!--   <br><br>
  Robotic agents are becoming more competent and are introduced in
  more complex environments. They are leaving the strict industrial
  settings and entering unstructured spaces like warehouses, care
  homes, and households. Acting in these new environments requires
  robots that are able to plan and execute complex tasks, and
  interact with people efficiently. In addition, in many of these new
  environments the robots will need to interact with people that do
  not have any formal robotics training. For humans, the most natural
  and intuitive way to communicate and provide information is through
  natural language. So, developing robots that are able to interpret
  and disambiguate sentences in natural language in order to follow
  instructions and receive or ask for feedback will provide great
  value to society. Additionally, robots in unstructured environments
  that are commanded to perform complex long-horizon tasks, will need
  to posses advanced reasoning and planning capabilities.  -->

  </p>

  <br>
  <h1 id="approach">Language in Robotic Manipulation
  <a hidden class="anchor" href="#approach">#</a></h1>
  <p>
    We can identify two central approaches to using language
    in robotic manipulation: 1) language-conditioned approaches
    (<a class="one" href="https://arxiv.org/pdf/1906.03926.pdf">Luketina 
    et al. 2019</a>) and 2) approaches that use language 
    as middleware 
    (<a class="one" href="https://www.youtube.com/watch?v=nxFwcfEp0II">Andy Zeng and Jacky Liang, 2022</a>). 
    Language-conditioned approaches, as their name 
    implies condition some component of the method on an
    instruction. For example, in reinforcement learning the 
    policy can be conditioned on a language command, or in planning
    scenarios the planner. In general, in language-conditioned methods,
    the language command is pre-processed by a pre-trained LLM, 
    and the output embedding is used as input to some part of the 
    low-level action generation method, be it an RL policy or a 
    planner. These methods are primarily utilized for executing 
    simple tasks from natural language instructions 
    that do not require complex task planning. On the other hand, 
    several approaches use language and LLMs in an intermediate 
    step during the planning process. For example, they leverage 
    the reasoning abilities of LLMs to break down complex high-level
    tasks in sequences of easy-to-execute subtasks, they use LLMs 
    to directly generate code that a low-level policy can execute,
    or they use language as a communication layer, to transfer 
    information between multimodal LMs. In this case, more complex 
    tasks can be performed that require high-level planning. 

  </p>

  <br>
  <h2 id="conditioned">Language Conditioned Control<a hidden class="anchor" href="#conditioned">#</a></h2>
  <p>

    In this case, natural language is used to specify the target
    task that the agent needs to perform. Task specification is a
    fundamental problem in robotic manipulation and human-robot 
    interaction. Using natural language to specify tasks provides 
    an intuitive way for humans to communicate with robots and 
    can facilitate human-robot collaboration. 
  </p>
  
    <br>
    <img src="assets/project_llms/lang_cond_ctrl.png" style="width:95%;border-radius:10px" class=center>
    <figcaption>Fig. 1. Simplified representation of the different language-conditioned control approaches. </figcaption>
    <br>

  <p>
    <b>Policy conditioning.</b> 
    The dominant approach in language-conditioned control 
    is to directly condition the action generation policy
    on a natural language instruction 
    (<a class="one" href="https://arxiv.org/pdf/2212.06817.pdf">Brohan et al. 2022</a>,
    <a class="one" href="https://arxiv.org/pdf/2010.12083.pdf">Stepputtis et al. 2020</a>). Typically, the 
    instruction is tokenized and an embedding is extracted
    using a pre-trained LLM. Models that are pre-trained
    on large amounts of data enable the agent to disambiguate 
    synonyms in instructions and handle instructions in 
    multiple languages (<a class="one" href="https://arxiv.org/pdf/2005.07648.pdf">Lynch and Sermanet, 2020</a>).
    
    The policies, are usually trained in a supervised fashion,
    e.g. using <a class="one" href="https://ai.stanford.edu/blog/learning-to-imitate/"> imitation learning</a>, 
    on recorded trajectories of the robot performing a task along
    with the corresponding instruction in natural language. 
    Usually, the data are gathered by users teleoperating the robot
    (<a class="one" href="https://arxiv.org/pdf/2210.06407.pdf">Lynch et al. 2022</a>). 
    Additional data augmentation techniques can be applied to learn
    more robust representations (<a class="one" href="https://arxiv.org/pdf/2109.12098.pdf">Shridhar et al. 2021</a>).

    <br> <br> 

    <b>Reward conditioning.</b> 
    Another approach, instead of directly using the language 
    instruction to condition the policy, is to train a 
    language-conditioned reward function (<a class="one" href="https://arxiv.org/pdf/2109.01115.pdf">Nair et al. 2021</a>). A multi-task policy
    can then be trained with reinforcement learning
    using this reward function. In a similar fashion to policy
    conditioning approaches the language instruction is fed
    into a pre-trained LLM and the embedding is then given to the 
    reward network. 

    <br> <br> 

    <b>Feedback conditioning.</b> 
    Finally, language instructions can be incorporated into the
    agent's planning loop, and be used as feedback to correct the 
    behavior of the robot online (<a class="one" href="https://arxiv.org/pdf/2204.05186.pdf">Sharma et al. 2022</a>). In this case,
    the embedding, extracted from the instruction, can be used to
    generate cost functions for the planner (<a class="one" href="https://arxiv.org/pdf/2204.05186.pdf">Sharma et al. 2022</a>)
    or to generate additional planning constraints to improve 
    the robot's trajectory (<a class="one" href="https://arxiv.org/pdf/2204.05186.pdf">Cui et al. 2023</a>).

  </p>

  <br>
  <h2 id="middleware">Language as Middleware
  <a hidden class="anchor" href="#middleware">#</a></h2>
  <p>
    Another way to leverage LLMs in robotic manipulation agents
    is to incorporate them into an internal stage of the planning 
    process. The main approaches include: 
    (1) using LLMs for high-level task planning, where the LLM must 
    decompose the task at hand into a step-by-step plan that involves 
    skills that the robot can execute using low-level control policies
    (<a class="one" href="https://arxiv.org/pdf/2201.07207.pdf">Huang et al. 2022a</a>, <a class="one" href="https://arxiv.org/pdf/2204.01691.pdf">Ahn et al. 2022</a>), 
    (2) using LLMs for code generation, i.e. the LLM's role is to 
    generate software code that will be directly executed by the 
    robot to perform the designated task
    (<a class="one" href="https://arxiv.org/pdf/2209.07753.pdf">Liang et al. 2022</a>, <a class="one" href="https://arxiv.org/pdf/2209.11302.pdf">Singh et al. 2023</a>), and 
    (3) integration of multimodal language models, in this case, 
    language serves as a means for various LMs to interact and 
    exchange information, facilitating communication among 
    different components of the system (<a class="one" href="https://arxiv.org/pdf/2303.03378.pdf">Driess et al. 2023</a>). 
    This approach enables agents to perform
    more complex and long-horizon tasks that require 
    planning and reasoning capabilities.
    </p>

    <br>
    <img src="assets/project_llms/language_middleware.png" 
    style="width:95%;border-radius:10px" class=center>
    <figcaption>
    Fig. 2. Simplified representation of the different 
    approaches using language as middleware. 
    </figcaption>
    <br>
    
    <p>
    <b>High-level Planning.</b>
    The main idea, is that LLMs that are trained on massive
    amounts of general data have been shown to contain enough
    internalized world knowledge on how to perform diverse 
    high-level tasks and can generalize to unseen ones
    (<a class="one" href="https://arxiv.org/pdf/2005.14165.pdf">Brown et al. 2020</a>). 
    Basically, the agent is presented with a task, given in the 
    form of an instruction expressed in natural language, and is
    asked to translate it into a detailed plan, i.e. sequence of
    subtasks, that the robot can execute to accomplish the goal
    (<a class="one" href="https://arxiv.org/pdf/2204.01691.pdf">Ahn et al. 2022</a>). 
    Typically, given the high-level instruction an input prompt 
    is designed to make the LLM produce a detailed plan of actions
    that need to be completed to perform the task by exploiting 
    the chain-of-thought reasoning scheme or few-shot prompting
    (<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10235949">Wake et al. 2023</a>).
    Several aspects need to be determined for this approach to 
    work. For example how to constrain the LLM to output sub-tasks 
    that the robotic agent can perform (<a class="one" href="https://arxiv.org/pdf/2201.07207.pdf">Huang et al. 2022b</a>), or how to match the output 
    of the LLM with a specific sub-policy.
    Additionally, several works have developed closed-loop
    approaches for planning with LLMs, in which the LLM is prompted
    with errors that the agent might encounter during the execution.
    By doing so, they use feedback from the environment and 
    from human interaction to re-plan the action sequences and
    correct its behavior (<a class="one" href="https://arxiv.org/pdf/2211.09935.pdf">Raman et al. 2022</a>).
    
    <br><br>
    <b>Code generation.</b>
    Instead of prompting the LLM to generate a plan in natural 
    language, we can also instruct it to explicitly generate the
    software code needed for the agent to perform a given task. 
    For example by including few-shot examples in the prompt 
    that contain instructions and the corresponding code
    (<a class="one" href="https://arxiv.org/pdf/2209.07753.pdf">Liang et al. 2023</a>). This
    way the LLM can take advantage of algorithmic structures 
    such as for-loops and if-statements to produce more efficient
    and successful plans (<a class="one" href="https://arxiv.org/pdf/2209.11302.pdf">Singh et al. 2023</a>). 

    <br><br>
    <b>Language model integration.</b>
    Finally, language can be used as a communication API between
    several multimodal language models, such as visual LMs (VLMs),
    audio LMs (ALMs), etc. This can be achieved through prompting, 
    i.e. using natural language to transfer information from one 
    model to the other, such as the visible objects or the sounds 
    that can be detected etc (<a class="one" href="https://arxiv.org/pdf/2204.00598.pdf">Zeng et al. 2022</a>). Additionally, another option is to use 
    end-to-end multimodal LMs that use both text and images to
    complete embodied decision-making tasks (<a class="one" href="https://arxiv.org/pdf/2303.03378.pdf">Driess et al. 2023</a>).

  </p>

  <br>
  <h1 id="discussion">Discussion<a hidden class="anchor" href="#discussion">#</a></h1>
  <p>
    Using language to instruct robots offers a more intuitive way for
    task specification allowing non-expert operators, such as the elderly,
    to interact with robots. Additionally, the robustness of LLMs to 
    synonyms in instructions and their ability to handle multilingual 
    commands broadens the use cases of this approach. However, 
    language-conditioned control has several limitations. Most methods 
    rely on a supervised training scheme (e.g. imitation learning) that
    requires large and well-balanced datasets, labeled with language 
    instructions that can include only a fixed set of objects and tasks. 
    As a consequence, these methods do not generalize well to 
    out-of-distribution situations and are hard to scale.
    <br>
    <br>
    Agents that use language in an intermediate layer for planning
    and reasoning are able to handle high-level complex instructions. 
    However, these approaches inherit the limitations of LLMs, including
    their biases and dependencies. For instance, LLMs commonly suffer 
    from hallucinations, which in this case can cause the planners to 
    suggest skills that are not available in the robot’s skill set. On 
    top of that, the range of available low-level skills that the robot
    can perform significantly constraints the number of high-level tasks 
    that can be successfully executed. Finally, these models continue to 
    struggle with instructions that are substantially more complex
    than the few-show examples that are presented in the prompts.
    <br>
    <br>  
    Integrating pre-trained LLMs with robotic agents has helped address 
    many challenging problems in robotic manipulation but has also complicated
    their societal impact. LLMs are trained on massive datasets containing 
    human-generated data collected from the internet that are hard to reason 
    about. Additionally, their black-box nature, as in most deep learning 
    models, offers little explainability over the way their outputs are 
    generated. This can potentially introduce biases in the agent’s behavior 
    that are present in the LLM’s dataset. Moreover, the misalignment of the
    training objective of most LLMs, which is some form of self-supervised
    objective, like predicting missing words without accounting for their 
    meaning, and their actual purpose, which is to operate in human-centered
    environments and interact with humans creates important risks. In general,
    there are several intangible risks associated with the use of
    pre-trained LLMs that are magnified when these models are coupled with
    robotic agents that can act in real-world environments. In addition, 
    several practical vulnerabilities have been identified that make LLM 
    potential attack targets by malicious parties, such as adversarial
    user inputs (<a class="one" href="https://arxiv.org/pdf/2211.09527.pdf">Perez and Ribeiro, 2022</a>), 
    where the operator of the robot can manipulate the LLM to exhibit 
    undesired behavior, and indirect prompt injection 
    (<a class="one" href="https://arxiv.org/pdf/2302.12173">Greshake et al. 2023</a>).
  </p>
  
  <br>
  <h1 id="case_study">Case Study<a hidden class="anchor" href="#case_study">#</a></h1>
  <p>
    As a case study for the project, we implemented an LLM enabled robotic 
    agent in a simulated environment based on the <a class="one" href="https://code-as-policies.github.io/">Code-as-Policies</a>
    method. We developed our environment in the IsaacGym simulator, and we
    we used the Panda arm 7DoF manipulator equipped with a parallel gripper. 
    The environment contains two tables, a cabinet, and a few household 
    objects from the YCB and Google scanned objects datasets. The full code
    along with the prompts can be found in this 
    <a class="one" href="https://github.com/el-cangrejo/isaac_gpt">repository</a>.
  </p>  
  
  <br>
  <img src="assets/project_llms/environment.png" border=2 style="width:60%;border-radius:20px" class=center>
  <figcaption>Fig. 3. Snapshot of simulated environment. </figcaption>
  <br>

  <p>    
    To control the robot to perform basic manipulation primitives we 
    developed an API that includes basic actions such as grasping an
    object, opening the drawer, etc. The low-level actions, i.e. the 
    joint angle positions that the agent needs to execute an action
    primitive, e.g. picking up an object, are calculated using the 
    inverse kinematics of the manipulator, while the grasp poses for
    the various objects are predefined. The agent is presented with 
    the following prompt that includes instructions and example code
    to execute them: 
  </p>
  <br> 
    <figcaption>Prompt for LLM.</figcaption>
  <div class="scroll-box">
  <p>
    ##### Write a Python function for a robot to execute the instruction given in the comment <br>
    <br>
    ## Put the ball in the box<br>
    items = ['ball']<br>
    places = ['box']<br>
    def put_ball_in_box():<br>
        box_pos = robot.get_object_pos('box')<br>
        robot.put_first_on_second('ball', box_pos)<br>
    put_ball_in_box()<br>
    <br>
    ## Put the cube in the drawer<br>
    items = ['ball', 'cube']<br>
    places = ['box', 'drawer']<br>
    def put_cube_and_ball_in_drawer():<br>
        robot.open_drawer()<br>
        drawer_pos = robot.get_object_pos('drawer')<br>
        robot.put_first_on_second('cube', drawer_pos)<br>
        robot.put_first_on_second('ball', drawer_pos)<br>
        robot.close_drawer()<br>
    put_cube_and_ball_in_drawer()<br>
    <br>
    ## Put the cube next to the sphere<br>
    items = ['ball', 'cube']<br>
    places = ['box', 'drawer']<br>
    def put_cube_next_to_ball():<br>
        ball_pos = robot.get_object_pos('ball')<br>
        target_pos = ball_pos + np.array([0., 0.1, 0.])<br>
        robot.put_first_on_second('cube', target_pos)<br>
    put_cube_next_to_ball()<br>
    <br>
    ## Put the cube in front of the sphere<br>
    items = ['ball', 'cube']<br>
    places = ['box', 'drawer']<br>
    def put_cube_front_of_ball():<br>
        ball_pos = robot.get_object_pos('ball')<br>
        target_pos = ball_pos + np.array([0.15, 0., 0.])<br>
        robot.put_first_on_second('cube', target_pos)<br>
    put_cube_front_of_ball()<br>
    <br>
    ## Put the ball right of the cube<br>
    items = ['ball', 'cube']<br>
    places = ['box', 'drawer']<br>
    def put_cube_and_ball_in_drawer():<br>
        cube_pos = robot.get_object_pos('cube')<br>
        target_pos = cube_pos + np.array([0.0, 0.15, 0.])<br>
        robot.put_first_on_second('ball', target_pos)<br>
    put_ball_right_of_cube()<br>
    <br>
    ## Put the first three items in the drawer<br>
    items = ['ball', 'cube_1', 'cube_2', 'cylinder_1', 'cylinder_2']<br>
    places = ['box', 'drawer']<br>
    def put_first_three_items_in_drawer():<br>
        robot.open_drawer()<br>
        drawer_pos = robot.get_object_pos('drawer')<br>
        for i in range(3):<br>
          robot.put_first_on_second(item[i], drawer_pos)<br>
        robot.close_drawer()<br>
    put_first_three_items_in_drawer()<br>
    <br>
    ## Put the all cubes in the drawer<br>
    items = ['ball', 'cube_1', 'cube_2', 'cylinder_1', 'cylinder_2']<br>
    places = ['box', 'drawer']<br>
    def put_all_cubes_in_drawer():<br>
        robot.open_drawer()<br>
        drawer_pos = robot.get_object_pos('drawer')<br>
        for item in ['cube_1', 'cube_2']:<br>
          robot.put_first_on_second(item, drawer_pos)<br>
        robot.close_drawer()<br>
    put_all_cubes_in_drawer()<br>
    <br>
    ## INSTRUCTION<br>
    items = ['banana', 'pear', 'coke_can', 'meat_can', 'orange']<br>
    places = ["coke_can", "pear", "banana", "meat_can", "orange", "bowl", "drawer", "table_1", "table_2"]<br>
  </p>  
  </div>

  <p>
    We assume that the agent has access to a list of the available 
    objects and locations (e.g. the drawer) in the scene as well as
    their 3D positions. We used the 
    <a class="one" href="https://platform.openai.com/docs/models/gpt-3-5">text-davinci-003</a> 
    from <a class="one" href="https://openai.com/">OpenAI</a> to 
    generate Python code. The instruction was inserted as a comment
    in the final line of the prompts and the output of the model was
    directly executed by the agent. The agent can follow simple 
    instructions such as: <em><instr> Put the coke in the bowl and the pear in the drawer.</instr></em>
  </p>

  
  <br>
  <img src="assets/project_llms/demo2_small.gif" border=2 style="width:60%;border-radius:10px" class=center>
  <figcaption>Fig. 4. Execution for instruction : Put the coke in the bowl
  and the pear in the drawer. </figcaption>
  <br>
  
  <p> 
    The agent is also able to follow more complex instructions 
    that require reasoning about the objects, such as: 
    <em><instr>Put all the fruits in the bowl.</instr></em> 
    The output of the model for this instruction was the 
    following:
  </p>


 <br>
<!-- HTML generated using hilite.me --><div style="background: #f8f8f8; overflow:auto;width:auto;border-width:.2em .2em .2em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%; font-size: 14px;"><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">put_all_fruits_in_bowl</span>():
  bowl_pos <span style="color: #666666">=</span> robot<span style="color: #666666">.</span>get_object_pos(<span style="color: #BA2121">&#39;bowl&#39;</span>)
  <span style="color: #008000; font-weight: bold">for</span> item <span style="color: #AA22FF; font-weight: bold">in</span> [<span style="color: #BA2121">&#39;banana&#39;</span>, <span style="color: #BA2121">&#39;pear&#39;</span>, <span style="color: #BA2121">&#39;orange&#39;</span>]:
    robot<span style="color: #666666">.</span>put_first_on_second(item, bowl_pos)

put_all_fruits_in_bowl()
</pre></div>

<br>

  <p> 
    The execution of this code can be seen bellow.
  </p> 

  <br>
  <img src="assets/project_llms/demo1_small.gif" border=2 style="width:60%;border-radius:10px" class=center>
  <figcaption>Fig. 5. Execution for instruction : Put all the fruits in the bowl. </figcaption>
  <br>

  <br>
  <h1 id="references">References<a hidden class="anchor" href="#references">#</a></h1>
    [1] Luketina et al. <a href="https://arxiv.org/pdf/1906.03926.pdf">
        "A Survey of Reinforcement Learning Informed by Natural Language."</a> ArXiv, 2019.

    <br> <br>
    [2] Andy Zeng and Jacky Liang. <a href="https://www.youtube.com/watch?v=nxFwcfEp0II">
        "Language as robot middleware"</a> Youtube, 2022.

    <br> <br>
    [3] Lynch, Corey and Pierre Sermanet. <a href="https://arxiv.org/pdf/2005.07648.pdf">“Language Conditioned Imitation Learning Over Unstructured Data.”</a> RSS, 2020.

    <br> <br>
    [4] Stepputtis et al. <a href="https://arxiv.org/pdf/2010.12083.pdf">“Language-Conditioned Imitation Learning for Robot Manipulation Tasks.”</a> ArXiv, 2020.

    <br> <br>
    [5] Lynch et al. <a href="https://arxiv.org/pdf/2210.06407.pdf">“Interactive Language: Talking to Robots in Real Time.”</a> 
    ArXiv, 2022.


    <br> <br>
    [6] Brohan et al. <a href="https://arxiv.org/pdf/2212.06817.pdf">“RT-1: Robotics Transformer for Real-World Control at Scale.”</a> ArXiv, 2022.


    <br> <br>
    [7] Shridhar et al. <a href="https://arxiv.org/pdf/2109.12098.pdf">“CLIPort: What and Where Pathways for Robotic Manipulation.”</a> ArXiv, 2021

    <br> <br>
    [8] Nair et al. <a href="https://arxiv.org/pdf/2109.01115.pdf">“Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation.”</a> CoRL, 2021.

    <br> <br>
    [9] Sharma et al. <a href="https://arxiv.org/pdf/2204.05186.pdf">“Correcting Robot Plans with Natural Language Feedback.”</a> ArXiv, 2022.

    <br> <br>
    [10] Cui et al. <a href="https://arxiv.org/pdf/2204.05186.pdf">“No, to the Right: Online Language Corrections for Robotic Manipulation via Shared Autonomy.”</a> HRI, 2023.

    <br> <br>
    [11] Huang et al. <a href="https://arxiv.org/pdf/2207.05608.pdf">“Inner Monologue: Embodied Reasoning through Planning with Language Models.”</a> CoRL, 2022a.

    <br> <br>
    [12] Wake et al. <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10235949">“ChatGPT Empowered Long-Step Robot Control in Various Environments: A Case Application.”</a> IEEE Access, 2023.

    <br> <br>
    [13] Ahn et al. <a href="https://arxiv.org/pdf/2204.01691.pdf">“Do As I Can, Not As I Say: Grounding Language in Robotic Affordances.”</a> CoRL, 2022.

    <br> <br>
    [14] Huang et al. <a href="https://arxiv.org/pdf/2201.07207.pdf">“Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents.”</a> ArXiv, 2022b.

    <br> <br>
    [15] Raman et al. <a href="https://arxiv.org/pdf/2211.09935.pdf">“CAPE: Corrective Actions from Precondition Errors using Large Language Models.”</a> ArXiv, 2022.

    <br> <br>
    [16] Liang et al. <a href="https://arxiv.org/pdf/2209.07753.pdf">“Code as Policies: Language Model Programs for Embodied Control.”</a> ICRA, 2023.

    <br> <br>
    [17] Singh et al. <a href="https://arxiv.org/pdf/2209.11302.pdf">“ProgPrompt: Generating Situated Robot Task Plans using Large Language Models.”</a> ICRA, 2023.

    <br> <br>
    [18] Driess et al. <a href="https://arxiv.org/pdf/2303.03378.pdf">
    "Palm-e: An embodied multimodal language model."</a> ArXiv, 2023.
    
    <br> <br>
    [19] Brown et al. <a href="https://arxiv.org/pdf/2005.14165.pdf">“Language Models are Few-Shot Learners.”</a>  ArXiv, 2020.


    <br> <br>
    [20] Zeng et al.  <a href="https://arxiv.org/pdf/2204.00598.pdf">“Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language.”</a> ArXiv, 2022.

    <br> <br>
    [21] Perez and Ribeiro. <a href="https://arxiv.org/pdf/2211.09527.pdf">“Ignore Previous Prompt: Attack Techniques For Language Models.”</a>  ArXiv, 2022.

    <br> <br>
    [22] Greshake et al. <a href="https://arxiv.org/pdf/2302.12173">“Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection.”</a>  ACM Workshop on Artificial Intelligence and Security, 2023.

  <button onclick="parent.location='project_flows.html'" id="prevBtn"
    class="button" title="Go to previous">
    <i class="fa fa-chevron-left fa-fw"></i>
  </button> 

  <button onclick="topFunction()" id="topBtn" class="button" title="Go to top">
    <i class="fa fa-chevron-up fa-fw"></i>
  </button> 

  <button onclick="parent.location='project_repair.html'" id="nextBtn" class="button" title="Go to next">
    <i class="fa fa-chevron-right fa-fw"></i>
  </button> 

  <button onclick="parent.location='index.html'" id="homeBtn" class="button" title="Go to home">
    <i class="fa fa-home fa-fw"></i>
  </button> 
</div>
<script src="javascripts/scale.fix.js"></script>
<script src="javascripts/scripts.js"></script>

</body>
</html>
